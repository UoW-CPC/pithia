tosca_definitions_version: tosca_simple_yaml_1_2
imports:
- https://raw.githubusercontent.com/micado-scale/tosca/develop/micado_types.yaml
repositories:
  docker_hub: https://hub.docker.com/
description: 'Generated from K8s manifests: deployment.yaml'
topology_template:
  node_templates:
    kube-state-metrics-clusterrole:
      type: tosca.nodes.MiCADO.Kubernetes
      interfaces:
        Kubernetes:
          create:
            inputs:
              apiVersion: rbac.authorization.k8s.io/v1
              kind: ClusterRole
              metadata:
                labels:
                  app.kubernetes.io/name: kube-state-metrics
                  app.kubernetes.io/version: v1.8.0
                name: kube-state-metrics
              rules:
              - apiGroups:
                - ''
                resources:
                - configmaps
                - secrets
                - nodes
                - pods
                - services
                - resourcequotas
                - replicationcontrollers
                - limitranges
                - persistentvolumeclaims
                - persistentvolumes
                - namespaces
                - endpoints
                verbs:
                - list
                - watch
              - apiGroups:
                - extensions
                resources:
                - daemonsets
                - deployments
                - replicasets
                - ingresses
                verbs:
                - list
                - watch
              - apiGroups:
                - apps
                resources:
                - statefulsets
                - daemonsets
                - deployments
                - replicasets
                verbs:
                - list
                - watch
              - apiGroups:
                - batch
                resources:
                - cronjobs
                - jobs
                verbs:
                - list
                - watch
              - apiGroups:
                - autoscaling
                resources:
                - horizontalpodautoscalers
                verbs:
                - list
                - watch
              - apiGroups:
                - authentication.k8s.io
                resources:
                - tokenreviews
                verbs:
                - create
              - apiGroups:
                - authorization.k8s.io
                resources:
                - subjectaccessreviews
                verbs:
                - create
              - apiGroups:
                - policy
                resources:
                - poddisruptionbudgets
                verbs:
                - list
                - watch
              - apiGroups:
                - certificates.k8s.io
                resources:
                - certificatesigningrequests
                verbs:
                - list
                - watch
              - apiGroups:
                - storage.k8s.io
                resources:
                - storageclasses
                - volumeattachments
                verbs:
                - list
                - watch
              - apiGroups:
                - admissionregistration.k8s.io
                resources:
                - mutatingwebhookconfigurations
                - validatingwebhookconfigurations
                verbs:
                - list
                - watch
              - apiGroups:
                - networking.k8s.io
                resources:
                - networkpolicies
                verbs:
                - list
                - watch
    kube-state-metrics-clusterrolebinding:
      type: tosca.nodes.MiCADO.Kubernetes
      interfaces:
        Kubernetes:
          create:
            inputs:
              apiVersion: rbac.authorization.k8s.io/v1
              kind: ClusterRoleBinding
              metadata:
                labels:
                  app.kubernetes.io/name: kube-state-metrics
                  app.kubernetes.io/version: v1.8.0
                name: kube-state-metrics
              roleRef:
                apiGroup: rbac.authorization.k8s.io
                kind: ClusterRole
                name: kube-state-metrics
              subjects:
              - kind: ServiceAccount
                name: kube-state-metrics
                namespace: kube-system
    kube-state-metrics-serviceaccount:
      type: tosca.nodes.MiCADO.Kubernetes
      interfaces:
        Kubernetes:
          create:
            inputs:
              apiVersion: v1
              kind: ServiceAccount
              metadata:
                labels:
                  app.kubernetes.io/name: kube-state-metrics
                  app.kubernetes.io/version: v1.8.0
                name: kube-state-metrics
                namespace: kube-system
    kube-state-metrics-deployment:
      type: tosca.nodes.MiCADO.Kubernetes
      interfaces:
        Kubernetes:
          create:
            inputs:
              apiVersion: apps/v1
              kind: Deployment
              metadata:
                labels:
                  app.kubernetes.io/name: kube-state-metrics
                  app.kubernetes.io/version: v1.8.0
                name: kube-state-metrics
                namespace: kube-system
              spec:
                replicas: 1
                selector:
                  matchLabels:
                    app.kubernetes.io/name: kube-state-metrics
                template:
                  metadata:
                    labels:
                      app.kubernetes.io/name: kube-state-metrics
                      app.kubernetes.io/version: v1.8.0
                  spec:
                    tolerations:
                    - key: node-role.kubernetes.io/master
                      effect: NoSchedule
                    containers:
                    - image: quay.io/coreos/kube-state-metrics:v1.8.0
                      livenessProbe:
                        httpGet:
                          path: /healthz
                          port: 8080
                        initialDelaySeconds: 5
                        timeoutSeconds: 5
                      name: kube-state-metrics
                      ports:
                      - containerPort: 8080
                        name: http-metrics
                      - containerPort: 8081
                        name: telemetry
                      readinessProbe:
                        httpGet:
                          path: /
                          port: 8081
                        initialDelaySeconds: 5
                        timeoutSeconds: 5
                    nodeSelector:
                      kubernetes.io/os: linux
                    serviceAccountName: kube-state-metrics
    kube-state-metrics-service:
      type: tosca.nodes.MiCADO.Kubernetes
      interfaces:
        Kubernetes:
          create:
            inputs:
              apiVersion: v1
              kind: Service
              metadata:
                labels:
                  app.kubernetes.io/name: kube-state-metrics
                  app.kubernetes.io/version: v1.8.0
                name: kube-state-metrics
                namespace: kube-system
              spec:
                clusterIP: None
                ports:
                - name: http-metrics
                  port: 8080
                  targetPort: http-metrics
                - name: telemetry
                  port: 8081
                  targetPort: telemetry
                selector:
                  app.kubernetes.io/name: kube-state-metrics

    #Don't Change these fields
    jupyterhub:
      type: tosca.nodes.MiCADO.Container.Application.Docker.Deployment
      properties:
        name: jupyterhub
        image: "balalior/micado-jupyterhub:1.0.4"
        env:
          - name: HUB_IP
            value: 0.0.0.0
          - name: SPAWNER_CLASS
            value: "kubespawner.KubeSpawner"
          - name: K8S_HUB_API_IP
            value: "jupyterhub-clusterip"
          - name: NFS_SERVER
            value: 18.133.255.231
          - name: NFS_PATH
            value: "/mnt/nfs_share/users"
        ports:
          - port: 8000
            #This is the public port to access the jupyterhub
            nodePort: 30001
          - port: 8081

      requirements:
       - host: jupyterhub-node
       - volume:
            node: nfs-volume
            relationship:
              type: tosca.relationships.AttachesTo
              properties:
                location: /data

    jupyterhub-node:
      type: tosca.nodes.MiCADO.EC2.Compute
      properties:
        region_name: eu-west-2
        image_id: ami-09a1e275e350acf38
        instance_type: t2.medium
        key_name: Dimitris-key
        #Allow all traffic to Jupyterhub
        security_group_ids:
          - sg-075395b33ac05ee99
        #Don't change these fields
        context:
          append: true
          cloud_config: |
            runcmd:
            - sudo apt-get install -y nfs-common

      interfaces:
        Occopus:
          create:
            inputs:
              endpoint: https://ec2.eu-west-2.amazonaws.com

    #Don't change these fields
    nfs-volume:
      type: tosca.nodes.MiCADO.Container.Volume.NFS
      properties:
        server: 18.133.255.231
        path: /mnt/nfs_share

  policies:
    - monitoring:
        type: tosca.policies.Monitoring.MiCADO
        properties:
          enable_container_metrics: true
          enable_node_metrics: true
    - scalability:
        type: tosca.policies.Scaling.MiCADO.VirtualMachine.jupyterhub
        targets: [ jupyterhub-node ]
        properties:
          sources:
          - 'kube-state-metrics.kube-system.svc.cluster.local:8080'
          constants:
            NODE_NAME: 'jupyterhub-node'
            SCALE_UP_IF_FREE_SLOTS_ARE_LESS_THAN: '2'
            SCALE_DOWN_IF_FREE_SLOTS_ARE_LESS_THAN: '4'
            PODS_PER_NODE_AVG: '6'
          queries:
            schedulable_nodes:  'count(sum(kube_node_status_condition{condition="Ready",status="true",node=~"jupyterhub-node.*"} == 1) by (node) and sum(max_over_time(kube_node_spec_unschedulable{node=~"jupyterhub-node.*"}[30s]) == 0) by (node)) OR on() vector(0)'
            unschedulable_nodes: 'count(sum(kube_node_status_condition{condition="Ready",status="true",node=~"jupyterhub-node.*"} == 1) by (node) and sum(max_over_time(kube_node_spec_unschedulable{node=~"jupyterhub-node.*"}[30s]) == 1) by (node)) OR on() vector(0)'
            #active_jhub_users: 'count(sum(max_over_time(kube_node_spec_unschedulable{node=~"jupyterhub-node.*"}[30s]) == 0) by (node) + sum(kube_pod_info{node=~"jupyterhub-node.*",pod=~"jupyter.*"}) by (node)) OR on() vector(0)'
          min_instances: 1
          max_instances: 5

policy_types:
  tosca.policies.Scaling.MiCADO.VirtualMachine.jupyterhub:
    derived_from: tosca.policies.Scaling.MiCADO
    description: base MiCADO policy defining data sources, constants, queries, alerts, limits and rules
    properties:
      alerts:
        type: list
        description: pre-define alerts for VM CPU
        default:
        - alert: jhub_overloaded
          expr: '((count(sum(kube_node_status_condition{condition="Ready",status="true",node=~"jupyterhub-node.*"} == 1) by (node) and sum(max_over_time(kube_node_spec_unschedulable{node=~"jupyterhub-node.*"}[30s]) == 0) by (node)) * {{PODS_PER_NODE_AVG}} OR on() vector(0)) - (sum(sum(max_over_time(kube_node_spec_unschedulable{node=~"jupyterhub-node.*"}[30s]) == 0) by (node) + sum(kube_pod_info{node=~"jupyterhub-node.*",pod=~"jupyter-.*"}) by (node)) OR on() vector(0))) < {{SCALE_UP_IF_FREE_SLOTS_ARE_LESS_THAN}}'
          for: 1m
        - alert: jhub_underloaded
          expr: '((count(sum(kube_node_status_condition{condition="Ready",status="true",node=~"jupyterhub-node.*"} == 1) by (node) and sum(max_over_time(kube_node_spec_unschedulable{node=~"jupyterhub-node.*"}[30s]) == 0) by (node)) * {{PODS_PER_NODE_AVG}} OR on() vector(0)) - (sum(sum(max_over_time(kube_node_spec_unschedulable{node=~"jupyterhub-node.*"}[30s]) == 0) by (node) + sum(kube_pod_info{node=~"jupyterhub-node.*",pod=~"jupyter-.*"}) by (node)) OR on() vector(0))) > {{SCALE_DOWN_IF_FREE_SLOTS_ARE_LESS_THAN}}'
          for: 1m
        - alert: jhub_node_unschedulable
          expr: '(count(sum(kube_node_status_condition{condition="Ready",status="true",node=~"jupyterhub-node.*"} == 1) by (node) and sum(max_over_time(kube_node_spec_unschedulable{node=~"jupyterhub-node.*"}[30s]) == 1) by (node)) OR on() vector(0)) > 0'
        required: true
      scaling_rule:
        type: string
        description: pre-define scaling rule for VM CPU
        default: |
          # Application status
          print("JupyterHub policy")
          print("Current status:")
          print("MiCADO, active JHUB nodes: {}".format(len(m_nodes)))
          print("MiCADO, required JHUB nodes: {}".format(m_node_count))
          print("MiCADO, time since required JHUB node count changed: {}".format(m_time_since_node_count_changed))
          print("Prometheus, schedulable  nodes: {}".format(schedulable_nodes))
          print("Prometheus, unschedulable  nodes: {}".format(unschedulable_nodes))
          #print("Prometheus, JHUB users: {}".format(active_jhub_users))

          # Query JHUB nodes and pods from kubernetes
          kube = pykube.HTTPClient(pykube.KubeConfig.from_file("/root/.kube/config"))
          nodes = pykube.Node.objects(kube).filter(selector={"micado.eu/node_type":"jupyterhub-node"})
          pods = pykube.Pod.objects(kube).filter(selector={"app":"jupyterhub"})

          # Create two dictionaries one for schedulable and one for unschedulable nodes
          # for key use the name of the nodes and for values the sum of the running pods on each node
          print("Collecting JHUB nodes and pods information...")
          pods_per_schedulable_nodes = dict()
          pods_per_unschedulable_nodes = dict()
          for node in nodes:
              if node.obj['status']['conditions'][4].get('status') and node.obj['status']['conditions'][4].get('status') == "True":
                if node.obj["spec"].get("unschedulable"):
                  pods_per_unschedulable_nodes[node.obj["metadata"].get("name")] = 0
                else:
                  pods_per_schedulable_nodes[node.obj["metadata"].get("name")] = 0
          for pod in pods:
            if pod.obj["spec"].get("nodeName") in pods_per_unschedulable_nodes:
              pods_per_unschedulable_nodes[pod.obj["spec"].get("nodeName")] += 1
            elif pod.obj["spec"].get("nodeName") in pods_per_schedulable_nodes:
              pods_per_schedulable_nodes[pod.obj["spec"].get("nodeName")] += 1

          print("Schedulable nodes:")
          print(pods_per_schedulable_nodes)
          if pods_per_schedulable_nodes:
            print("least utilised schedulable node: {}".format(min(pods_per_schedulable_nodes, key=pods_per_schedulable_nodes.get)))
          print("Unschedulable nodes:")
          print(pods_per_unschedulable_nodes)
          if pods_per_unschedulable_nodes:
            print("most utilised unschedulable node: {}".format(max(pods_per_unschedulable_nodes, key=pods_per_unschedulable_nodes.get)))

          print("Checking if policy can be applied...")
          # Checking if MiCADO performs changes in the cluster nodes or if there are unschedulable nodes
          if (len(m_nodes) <= m_node_count and m_time_since_node_count_changed > 60) or pods_per_unschedulable_nodes:
            print("Applying...")

            # Checking if Prometheus fires an overloaded alert
            if jhub_overloaded:
              print("JupyterHub is overloaded")
              # If there are unschedulable nodes make SCHEDULABLE the most utilised node, else ADD a new node
              if pods_per_unschedulable_nodes:
                for node in nodes:
                  if node.obj["metadata"].get("name") == max(pods_per_unschedulable_nodes, key=pods_per_unschedulable_nodes.get):
                    print("Making the most utilised node schedulable: {}".format(node))
                    node.uncordon()
                    break
              else:
                print("Adding a new node..")
                m_node_count+=1

            # Checking if Prometheus fires an underloaded alert
            elif jhub_underloaded and len(m_nodes)>1:
              print("JupyterHub is underloaded")
              # If there is an empty and unschedulable nodes REMOVE it, else make the most utilised node UNSCHEDULABLE
              if pods_per_unschedulable_nodes and min(pods_per_unschedulable_nodes.values()) == 0:
                for node in nodes:
                  if node.obj["metadata"].get("name") == min(pods_per_unschedulable_nodes, key=pods_per_unschedulable_nodes.get):
                    m_nodes_todrop.append(node.obj["status"]["addresses"][0].get("address"))
                    print("Removing a empty node: {}".format(node))
                    break
              else:
                for node in nodes:
                  node.obj["metadata"].get("name") == min(pods_per_schedulable_nodes, key=pods_per_schedulable_nodes.get)
                  print("Making the leas utilised node unschedulable: {}".format(node))
                  node.cordon()
                  break
            else:
              print("nothing to apply")
          else:
            print('Transient phase, skipping...')
        required: true
