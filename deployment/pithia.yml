# Pithia reference architecture.
# Applications:
#               1. JupyterHub     (lines  x-x)
#               2. Apache Hadoop  (lines  x-x)
#               3. SMARTEST       (lines  x-x)
# Supplementary tools:
#               1. Kube-state-metrics  (lines  x-x)

tosca_definitions_version: tosca_simple_yaml_1_2
imports:
- https://raw.githubusercontent.com/micado-scale/tosca/develop/micado_types.yaml
repositories:
  docker_hub: https://hub.docker.com/
description: 'Generated from K8s manifests: deployment.yaml'
topology_template:
  node_templates:
    kube-state-metrics-clusterrole:
      type: tosca.nodes.MiCADO.Kubernetes
      interfaces:
        Kubernetes:
          create:
            inputs:
              apiVersion: rbac.authorization.k8s.io/v1
              kind: ClusterRole
              metadata:
                labels:
                  app.kubernetes.io/name: kube-state-metrics
                  app.kubernetes.io/version: v1.8.0
                name: kube-state-metrics
              rules:
              - apiGroups:
                - ''
                resources:
                - configmaps
                - secrets
                - nodes
                - pods
                - services
                - resourcequotas
                - replicationcontrollers
                - limitranges
                - persistentvolumeclaims
                - persistentvolumes
                - namespaces
                - endpoints
                verbs:
                - list
                - watch
              - apiGroups:
                - extensions
                resources:
                - daemonsets
                - deployments
                - replicasets
                - ingresses
                verbs:
                - list
                - watch
              - apiGroups:
                - apps
                resources:
                - statefulsets
                - daemonsets
                - deployments
                - replicasets
                verbs:
                - list
                - watch
              - apiGroups:
                - batch
                resources:
                - cronjobs
                - jobs
                verbs:
                - list
                - watch
              - apiGroups:
                - autoscaling
                resources:
                - horizontalpodautoscalers
                verbs:
                - list
                - watch
              - apiGroups:
                - authentication.k8s.io
                resources:
                - tokenreviews
                verbs:
                - create
              - apiGroups:
                - authorization.k8s.io
                resources:
                - subjectaccessreviews
                verbs:
                - create
              - apiGroups:
                - policy
                resources:
                - poddisruptionbudgets
                verbs:
                - list
                - watch
              - apiGroups:
                - certificates.k8s.io
                resources:
                - certificatesigningrequests
                verbs:
                - list
                - watch
              - apiGroups:
                - storage.k8s.io
                resources:
                - storageclasses
                - volumeattachments
                verbs:
                - list
                - watch
              - apiGroups:
                - admissionregistration.k8s.io
                resources:
                - mutatingwebhookconfigurations
                - validatingwebhookconfigurations
                verbs:
                - list
                - watch
              - apiGroups:
                - networking.k8s.io
                resources:
                - networkpolicies
                verbs:
                - list
                - watch
    kube-state-metrics-clusterrolebinding:
      type: tosca.nodes.MiCADO.Kubernetes
      interfaces:
        Kubernetes:
          create:
            inputs:
              apiVersion: rbac.authorization.k8s.io/v1
              kind: ClusterRoleBinding
              metadata:
                labels:
                  app.kubernetes.io/name: kube-state-metrics
                  app.kubernetes.io/version: v1.8.0
                name: kube-state-metrics
              roleRef:
                apiGroup: rbac.authorization.k8s.io
                kind: ClusterRole
                name: kube-state-metrics
              subjects:
              - kind: ServiceAccount
                name: kube-state-metrics
                namespace: kube-system
    kube-state-metrics-serviceaccount:
      type: tosca.nodes.MiCADO.Kubernetes
      interfaces:
        Kubernetes:
          create:
            inputs:
              apiVersion: v1
              kind: ServiceAccount
              metadata:
                labels:
                  app.kubernetes.io/name: kube-state-metrics
                  app.kubernetes.io/version: v1.8.0
                name: kube-state-metrics
                namespace: kube-system
    kube-state-metrics-deployment:
      type: tosca.nodes.MiCADO.Kubernetes
      interfaces:
        Kubernetes:
          create:
            inputs:
              apiVersion: apps/v1
              kind: Deployment
              metadata:
                labels:
                  app.kubernetes.io/name: kube-state-metrics
                  app.kubernetes.io/version: v1.8.0
                name: kube-state-metrics
                namespace: kube-system
              spec:
                replicas: 1
                selector:
                  matchLabels:
                    app.kubernetes.io/name: kube-state-metrics
                template:
                  metadata:
                    labels:
                      app.kubernetes.io/name: kube-state-metrics
                      app.kubernetes.io/version: v1.8.0
                  spec:
                    tolerations:
                    - key: node-role.kubernetes.io/master
                      effect: NoSchedule
                    containers:
                    - image: quay.io/coreos/kube-state-metrics:v1.8.0
                      livenessProbe:
                        httpGet:
                          path: /healthz
                          port: 8080
                        initialDelaySeconds: 5
                        timeoutSeconds: 5
                      name: kube-state-metrics
                      ports:
                      - containerPort: 8080
                        name: http-metrics
                      - containerPort: 8081
                        name: telemetry
                      readinessProbe:
                        httpGet:
                          path: /
                          port: 8081
                        initialDelaySeconds: 5
                        timeoutSeconds: 5
                    nodeSelector:
                      kubernetes.io/os: linux
                    serviceAccountName: kube-state-metrics
    kube-state-metrics-service:
      type: tosca.nodes.MiCADO.Kubernetes
      interfaces:
        Kubernetes:
          create:
            inputs:
              apiVersion: v1
              kind: Service
              metadata:
                labels:
                  app.kubernetes.io/name: kube-state-metrics
                  app.kubernetes.io/version: v1.8.0
                name: kube-state-metrics
                namespace: kube-system
              spec:
                clusterIP: None
                ports:
                - name: http-metrics
                  port: 8080
                  targetPort: http-metrics
                - name: telemetry
                  port: 8081
                  targetPort: telemetry
                selector:
                  app.kubernetes.io/name: kube-state-metrics
    #Don't Change these fields
    jupyterhub:
      type: tosca.nodes.MiCADO.Container.Application.Docker.Deployment
      properties:
        name: jupyterhub
        image: "balalior/micado-jupyterhub:1.0.4"
        env:
          - name: HUB_IP
            value: 0.0.0.0
          - name: SPAWNER_CLASS
            value: "kubespawner.KubeSpawner"
          - name: K8S_HUB_API_IP
            value: "jupyterhub-clusterip"
          - name: NFS_SERVER
            value: 18.133.255.231
          - name: NFS_PATH
            value: "/mnt/nfs_share/users"
        ports:
          - port: 8000
            #This is the public port to access the jupyterhub
            nodePort: 30001
          - port: 8081

      requirements:
       - host: jupyterhub-node
       - volume:
            node: nfs-volume
            relationship:
              type: tosca.relationships.AttachesTo
              properties:
                location: /data

    jupyterhub-node:
      type: tosca.nodes.MiCADO.EC2.Compute
      properties:
        region_name: eu-west-2
        image_id: ami-09a1e275e350acf38
        instance_type: t2.medium
        key_name: Dimitris-key
        #Allow all trafic to Jupyterhub
        security_group_ids:
          - sg-075395b33ac05ee99
        #Don't change these fields
        context:
          append: true
          cloud_config: |
            runcmd:
            - sudo apt-get install -y nfs-common

      interfaces:
        Occopus:
          create:
            inputs:
              endpoint: https://ec2.eu-west-2.amazonaws.com

    #Don't change these fields
    nfs-volume:
      type: tosca.nodes.MiCADO.Container.Volume.NFS
      properties:
        server: 18.133.255.231
        path: /mnt/nfs_share

  policies:
    - monitoring:
        type: tosca.policies.Monitoring.MiCADO
        properties:
          enable_container_metrics: true
          enable_node_metrics: true
    - scalability:
        type: tosca.policies.Scaling.MiCADO.VirtualMachine.CPU.jupyterhub
        targets: [ jupyterhub-node ]
        properties:
          sources:
          - 'kube-state-metrics.kube-system.svc.cluster.local:8080'
          constants:
            NODE_NAME: 'jupyterhub-node'
            SCALE_UP_IF_FREE_SLOTS_ARE_LESS_THAN: '2'
            SCALE_DOWN_IF_FREE_SLOTS_ARE_LESS_THAN: '4'
            PODS_PER_NODE_LIMIT: '6'
          min_instances: 1
          max_instances: 5

policy_types:
  tosca.policies.Scaling.MiCADO.VirtualMachine.CPU.jupyterhub:
    derived_from: tosca.policies.Scaling.MiCADO
    description: base MiCADO policy defining data sources, constants, queries, alerts, limits and rules
    properties:     
      alerts:
        type: list
        description: pre-define alerts for VM CPU
        default:
        - alert: jhub_overloaded
          expr: '(count(node_load1{node="jupyterhub-node"}) * {{PODS_PER_NODE_LIMIT}}) - count(container_last_seen{container_label_app="jupyterhub"}) < {{SCALE_UP_IF_FREE_SLOTS_ARE_LESS_THAN}}'
          for: 1m
        - alert: jhub_underloaded
          expr: '(count(node_load1{node="jupyterhub-node"}) * {{PODS_PER_NODE_LIMIT}}) - count(container_last_seen{container_label_app="jupyterhub"}) > {{SCALE_DOWN_IF_FREE_SLOTS_ARE_LESS_THAN}}'
          for: 1m
        required: true
      scaling_rule:
        type: string
        description: pre-define scaling rule for VM CPU
        default: |
          print("active nodes:",len(m_nodes))
          print("required nodes:",m_node_count)
          print("time since node count changed:",m_time_since_node_count_changed)
          node_unschedulable = False
          # Query nodes and pods
          kube = pykube.HTTPClient(pykube.KubeConfig.from_file("/root/.kube/config"))
          nodes = pykube.Node.objects(kube).filter(selector={"micado.eu/node_type":"jupyterhub-node"})
          pods = pykube.Pod.objects(kube).filter(selector={"app":"jupyterhub"})
          # Checking if there is an unschedulable and ready node
          for node in nodes:
            print("node name:",node)
            print("unschedulable:",node.obj["spec"].get("unschedulable"))
            print("ready:",node.obj['status']['conditions'][4].get('status') == "True")
            if node.obj["spec"].get("unschedulable") and node.obj['status']['conditions'][4].get('status') == "True":
              node_unschedulable = True
              print("flag node_unschedulable enabled for:", node)
              break
          # Checking if MiCADO performs changes in the cluster nodes or there is a ready and unschedulable node
          print("checking if policy can be applied...")
          if (len(m_nodes) <= m_node_count and m_time_since_node_count_changed > 60) or node_unschedulable: 
            print("Applying policy..")  
            # Condition True if Prometheus fires an overloaded alert
            if jhub_overloaded:
              print("JupyterHub is overloaded")      
              print("searching for an unschedulable node...")
              # If a node is ready and unschedulable make it SCHEDULABLE
              for node in nodes:
                if node.obj["spec"].get("unschedulable") and node.obj['status']['conditions'][4].get('status') == "True":
                  node.uncordon()
                  print("making a node schedulable:", node )
                  print("no need to add another node.")
                  break
              # If no unschedulable node exists ADD a node   
              if not node_unschedulable:
                print("no unschedulable node found.")
                print("adding a new node..")
                m_node_count+=1
            # Condition True if Prometheus fires an underloaded alert
            elif jhub_underloaded and len(m_nodes)>1:
              print("JupyterHub is underloaded")
              # Count pods per node       
              pods_per_node = dict()
              for node in nodes:
                # If node is ready add it to the dict      
                if node.obj['status']['conditions'][4].get('status') == "True":
                  pods_per_node[node.obj["metadata"].get("name")] = 0 
              # Iterate pods to get there nodes
              for pod in pods:
                pods_per_node[pod.obj["spec"].get("nodeName")] += 1 
              print(pods_per_node)
              print("least utilised node:")
              print(min(pods_per_node, key=pods_per_node.get))
              for node in nodes:
                # If a node is ready, empty and unschedulable REMOVE an node       
                if node.obj['status']['conditions'][4].get('status') == "True" and pods_per_node[node.obj["metadata"].get("name")] == 0 and node.obj["spec"].get("unschedulable"):
                  print("removing an empty node:", node)
                  m_nodes_todrop.append(node.obj["status"]["addresses"][0].get("address"))
                  break
                # If no unschedulable node exists mark the least utilised ready node as UNSCHEDULABLE
                elif node.obj['status']['conditions'][4].get('status') == "True" and node.obj["metadata"].get("name") == min(pods_per_node, key=pods_per_node.get) and not node_unschedulable:
                  print("Making a node unschedulable:", node)
                  node.cordon() 
                  break
            else:
              print("nothing to apply, skipping policy...")   
          else:
            print('Transient phase, skipping policy...')
        required: true
